{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "You are a data scientist working for a school\n",
    "\n",
    "You are asked to predict the GPA of the current students based on the following provided data: \n",
    "\n",
    " 0   StudentID  int64  \n",
    " 1   Age    int64  \n",
    " 2   Gender int64  \n",
    " 3   Ethnicity  int64  \n",
    " 4   ParentalEducation  int64  \n",
    " 5   StudyTimeWeekly    float64\n",
    " 6   Absences   int64  \n",
    " 7   Tutoring   int64  \n",
    " 8   ParentalSupport    int64  \n",
    " 9   Extracurricular    int64  \n",
    " 10  Sports int64  \n",
    " 11  Music  int64  \n",
    " 12  Volunteering   int64  \n",
    " 13  GPA    float64\n",
    " 14  GradeClass float64\n",
    "\n",
    "The GPA is the Grade Point Average, typically ranges from 0.0 to 4.0 in most educational systems, with 4.0 representing an 'A' or excellent performance.\n",
    "\n",
    "The minimum passing GPA can vary by institution, but it's often around 2.0. This usually corresponds to a 'C' grade, which is considered satisfactory.\n",
    "\n",
    "You need to create a Deep Learning model capable to predict the GPA of a Student based on a set of provided features.\n",
    "The data provided represents 2,392 students.\n",
    "\n",
    "In this excersice you will be requested to create a total of three models and select the most performant one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Import Libraries\n",
    "\n",
    "First let's import the following libraries, if there is any library that you need and is not in the list bellow feel free to include it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Load Data\n",
    "\n",
    "- You will be provided with a cvs (comma separated value) file.\n",
    "- You will need to add that file into a pandas dataframe, you can use the following code as reference\n",
    "- The file will be available in canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudentID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>ParentalEducation</th>\n",
       "      <th>StudyTimeWeekly</th>\n",
       "      <th>Absences</th>\n",
       "      <th>Tutoring</th>\n",
       "      <th>ParentalSupport</th>\n",
       "      <th>Extracurricular</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Music</th>\n",
       "      <th>Volunteering</th>\n",
       "      <th>GPA</th>\n",
       "      <th>GradeClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19.833723</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.929196</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.408756</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.042915</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4.210570</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112602</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.028829</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.054218</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.672495</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.288061</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1006</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.191219</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.084184</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1007</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.601680</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.748237</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1008</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>15.424496</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.360143</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1009</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.562008</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.896819</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1010</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18.444466</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.573474</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   StudentID  Age  Gender  Ethnicity  ParentalEducation  StudyTimeWeekly  \\\n",
       "0       1001   17       1          0                  2        19.833723   \n",
       "1       1002   18       0          0                  1        15.408756   \n",
       "2       1003   15       0          2                  3         4.210570   \n",
       "3       1004   17       1          0                  3        10.028829   \n",
       "4       1005   17       1          0                  2         4.672495   \n",
       "5       1006   18       0          0                  1         8.191219   \n",
       "6       1007   15       0          1                  1        15.601680   \n",
       "7       1008   15       1          1                  4        15.424496   \n",
       "8       1009   17       0          0                  0         4.562008   \n",
       "9       1010   16       1          0                  1        18.444466   \n",
       "\n",
       "   Absences  Tutoring  ParentalSupport  Extracurricular  Sports  Music  \\\n",
       "0         7         1                2                0       0      1   \n",
       "1         0         0                1                0       0      0   \n",
       "2        26         0                2                0       0      0   \n",
       "3        14         0                3                1       0      0   \n",
       "4        17         1                3                0       0      0   \n",
       "5         0         0                1                1       0      0   \n",
       "6        10         0                3                0       1      0   \n",
       "7        22         1                1                1       0      0   \n",
       "8         1         0                2                0       1      0   \n",
       "9         0         0                3                1       0      0   \n",
       "\n",
       "   Volunteering       GPA  GradeClass  \n",
       "0             0  2.929196         2.0  \n",
       "1             0  3.042915         1.0  \n",
       "2             0  0.112602         4.0  \n",
       "3             0  2.054218         3.0  \n",
       "4             0  1.288061         4.0  \n",
       "5             0  3.084184         1.0  \n",
       "6             0  2.748237         2.0  \n",
       "7             0  1.360143         4.0  \n",
       "8             1  2.896819         2.0  \n",
       "9             0  3.573474         0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Student_performance_data _.csv')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Review you data:\n",
    "\n",
    "Make sure you review your data.\n",
    "Place special attention of null or empty values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StudentID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>ParentalEducation</th>\n",
       "      <th>StudyTimeWeekly</th>\n",
       "      <th>Absences</th>\n",
       "      <th>Tutoring</th>\n",
       "      <th>ParentalSupport</th>\n",
       "      <th>Extracurricular</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Music</th>\n",
       "      <th>Volunteering</th>\n",
       "      <th>GPA</th>\n",
       "      <th>GradeClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2392.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "      <td>2392.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2196.500000</td>\n",
       "      <td>16.468645</td>\n",
       "      <td>0.510870</td>\n",
       "      <td>0.877508</td>\n",
       "      <td>1.746237</td>\n",
       "      <td>9.771992</td>\n",
       "      <td>14.541388</td>\n",
       "      <td>0.301421</td>\n",
       "      <td>2.122074</td>\n",
       "      <td>0.383361</td>\n",
       "      <td>0.303512</td>\n",
       "      <td>0.196906</td>\n",
       "      <td>0.157191</td>\n",
       "      <td>1.906186</td>\n",
       "      <td>2.983696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>690.655244</td>\n",
       "      <td>1.123798</td>\n",
       "      <td>0.499986</td>\n",
       "      <td>1.028476</td>\n",
       "      <td>1.000411</td>\n",
       "      <td>5.652774</td>\n",
       "      <td>8.467417</td>\n",
       "      <td>0.458971</td>\n",
       "      <td>1.122813</td>\n",
       "      <td>0.486307</td>\n",
       "      <td>0.459870</td>\n",
       "      <td>0.397744</td>\n",
       "      <td>0.364057</td>\n",
       "      <td>0.915156</td>\n",
       "      <td>1.233908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1001.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1598.750000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.043079</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.174803</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2196.500000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.705363</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.893393</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2794.250000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.408410</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.622216</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3392.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>19.978094</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         StudentID          Age       Gender    Ethnicity  ParentalEducation  \\\n",
       "count  2392.000000  2392.000000  2392.000000  2392.000000        2392.000000   \n",
       "mean   2196.500000    16.468645     0.510870     0.877508           1.746237   \n",
       "std     690.655244     1.123798     0.499986     1.028476           1.000411   \n",
       "min    1001.000000    15.000000     0.000000     0.000000           0.000000   \n",
       "25%    1598.750000    15.000000     0.000000     0.000000           1.000000   \n",
       "50%    2196.500000    16.000000     1.000000     0.000000           2.000000   \n",
       "75%    2794.250000    17.000000     1.000000     2.000000           2.000000   \n",
       "max    3392.000000    18.000000     1.000000     3.000000           4.000000   \n",
       "\n",
       "       StudyTimeWeekly     Absences     Tutoring  ParentalSupport  \\\n",
       "count      2392.000000  2392.000000  2392.000000      2392.000000   \n",
       "mean          9.771992    14.541388     0.301421         2.122074   \n",
       "std           5.652774     8.467417     0.458971         1.122813   \n",
       "min           0.001057     0.000000     0.000000         0.000000   \n",
       "25%           5.043079     7.000000     0.000000         1.000000   \n",
       "50%           9.705363    15.000000     0.000000         2.000000   \n",
       "75%          14.408410    22.000000     1.000000         3.000000   \n",
       "max          19.978094    29.000000     1.000000         4.000000   \n",
       "\n",
       "       Extracurricular       Sports        Music  Volunteering          GPA  \\\n",
       "count      2392.000000  2392.000000  2392.000000   2392.000000  2392.000000   \n",
       "mean          0.383361     0.303512     0.196906      0.157191     1.906186   \n",
       "std           0.486307     0.459870     0.397744      0.364057     0.915156   \n",
       "min           0.000000     0.000000     0.000000      0.000000     0.000000   \n",
       "25%           0.000000     0.000000     0.000000      0.000000     1.174803   \n",
       "50%           0.000000     0.000000     0.000000      0.000000     1.893393   \n",
       "75%           1.000000     1.000000     0.000000      0.000000     2.622216   \n",
       "max           1.000000     1.000000     1.000000      1.000000     4.000000   \n",
       "\n",
       "        GradeClass  \n",
       "count  2392.000000  \n",
       "mean      2.983696  \n",
       "std       1.233908  \n",
       "min       0.000000  \n",
       "25%       2.000000  \n",
       "50%       4.000000  \n",
       "75%       4.000000  \n",
       "max       4.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2392 entries, 0 to 2391\n",
      "Data columns (total 15 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   StudentID          2392 non-null   int64  \n",
      " 1   Age                2392 non-null   int64  \n",
      " 2   Gender             2392 non-null   int64  \n",
      " 3   Ethnicity          2392 non-null   int64  \n",
      " 4   ParentalEducation  2392 non-null   int64  \n",
      " 5   StudyTimeWeekly    2392 non-null   float64\n",
      " 6   Absences           2392 non-null   int64  \n",
      " 7   Tutoring           2392 non-null   int64  \n",
      " 8   ParentalSupport    2392 non-null   int64  \n",
      " 9   Extracurricular    2392 non-null   int64  \n",
      " 10  Sports             2392 non-null   int64  \n",
      " 11  Music              2392 non-null   int64  \n",
      " 12  Volunteering       2392 non-null   int64  \n",
      " 13  GPA                2392 non-null   float64\n",
      " 14  GradeClass         2392 non-null   float64\n",
      "dtypes: float64(3), int64(12)\n",
      "memory usage: 280.4 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove the columns not needed for Student performance prediction\n",
    "\n",
    "- Choose only the columns you consider to be valuable for your model training.\n",
    "- For example, StudentID might not be a good feature for your model, and thus should be removed from your main dataset, which other columns should also be removed?\n",
    "- You can name that final dataset as 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ParentalEducation</th>\n",
       "      <th>StudyTimeWeekly</th>\n",
       "      <th>Absences</th>\n",
       "      <th>Tutoring</th>\n",
       "      <th>ParentalSupport</th>\n",
       "      <th>Extracurricular</th>\n",
       "      <th>Sports</th>\n",
       "      <th>Music</th>\n",
       "      <th>Volunteering</th>\n",
       "      <th>GPA</th>\n",
       "      <th>GradeClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>19.833723</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.929196</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15.408756</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.042915</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.210570</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112602</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.028829</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.054218</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>4.672495</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.288061</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>3</td>\n",
       "      <td>10.680555</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.455509</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>1</td>\n",
       "      <td>7.583217</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.279150</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>2</td>\n",
       "      <td>6.805500</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.142333</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>0</td>\n",
       "      <td>12.416653</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.803297</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>2</td>\n",
       "      <td>17.819907</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.140014</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2392 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ParentalEducation  StudyTimeWeekly  Absences  Tutoring  ParentalSupport  \\\n",
       "0                     2        19.833723         7         1                2   \n",
       "1                     1        15.408756         0         0                1   \n",
       "2                     3         4.210570        26         0                2   \n",
       "3                     3        10.028829        14         0                3   \n",
       "4                     2         4.672495        17         1                3   \n",
       "...                 ...              ...       ...       ...              ...   \n",
       "2387                  3        10.680555         2         0                4   \n",
       "2388                  1         7.583217         4         1                4   \n",
       "2389                  2         6.805500        20         0                2   \n",
       "2390                  0        12.416653        17         0                2   \n",
       "2391                  2        17.819907        13         0                2   \n",
       "\n",
       "      Extracurricular  Sports  Music  Volunteering       GPA  GradeClass  \n",
       "0                   0       0      1             0  2.929196         2.0  \n",
       "1                   0       0      0             0  3.042915         1.0  \n",
       "2                   0       0      0             0  0.112602         4.0  \n",
       "3                   1       0      0             0  2.054218         3.0  \n",
       "4                   0       0      0             0  1.288061         4.0  \n",
       "...               ...     ...    ...           ...       ...         ...  \n",
       "2387                1       0      0             0  3.455509         0.0  \n",
       "2388                0       1      0             0  3.279150         4.0  \n",
       "2389                0       0      0             1  1.142333         2.0  \n",
       "2390                0       1      1             0  1.803297         1.0  \n",
       "2391                0       0      0             1  2.140014         1.0  \n",
       "\n",
       "[2392 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "dataset = data.drop(columns=['StudentID', 'Age', 'Gender', 'Ethnicity'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Check if the columns has any null values:\n",
    "- Here you now have your final dataset to use in your model training.\n",
    "- Before moving foward review your data check for any null or empty value that might be needed to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2392 entries, 0 to 2391\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   ParentalEducation  2392 non-null   int64  \n",
      " 1   StudyTimeWeekly    2392 non-null   float64\n",
      " 2   Absences           2392 non-null   int64  \n",
      " 3   Tutoring           2392 non-null   int64  \n",
      " 4   ParentalSupport    2392 non-null   int64  \n",
      " 5   Extracurricular    2392 non-null   int64  \n",
      " 6   Sports             2392 non-null   int64  \n",
      " 7   Music              2392 non-null   int64  \n",
      " 8   Volunteering       2392 non-null   int64  \n",
      " 9   GPA                2392 non-null   float64\n",
      " 10  GradeClass         2392 non-null   float64\n",
      "dtypes: float64(3), int64(8)\n",
      "memory usage: 205.7 KB\n",
      "\n",
      "Number of NULL values: 0\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "dataset.info()\n",
    "\n",
    "print(\"\\nNumber of NULL values:\", dataset.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prepare your data for training and for testing set:\n",
    " - First create a dataset named X, with all columns but GPA. These are the features\n",
    " - Next create another dataset named y, with only GPA column. This is the label\n",
    " - If you go to your Imports, you will see the following import: **'from sklearn.model_selection import train_test_split'**\n",
    " - Use that *train_test_split* function to create: X_train, X_test, y_train and y_test respectively. Use X and y datasets as parameters. Other parameters to use are: Test Size = 0.2, Random State = 42.\n",
    " \n",
    " - Standarize your features (X_train and X_test) by using the StandardScaler (investigate how to use fit_transform and transform functions). This will help the training process by dealing with normilized data.\n",
    "\n",
    " Note: Your X_train shape should be around (1913, 10). This means the dataset has 10 columns which should be the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1913, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "X = dataset.drop(\"GPA\", axis=1)\n",
    "y = dataset[[\"GPA\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Define your Deep Neural Network.\n",
    "- This will be a Sequential Neural Network.\n",
    "- With a Dense input layer with 64 units, and input dimention of 10 and Relu as the activation function.\n",
    "- A Dense hidden layer with 32 units, and Relu as the activation function.\n",
    "- And a Dense output layer with 1 unit, do not define an activation function so it defaults to linear, suitable for regression tasks. e.g. Dense(1)\n",
    "\n",
    "This last part of the output layer is super important, since we want to predict the GPA, this means that we want a regression and not a classification. Linear activation function is best for regression and Sigmoid is best for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eryke\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,817</span> (11.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,817\u001b[0m (11.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,817</span> (11.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,817\u001b[0m (11.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=10, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Compile your Neural Network\n",
    "- Choose Adam as the optimizer\n",
    "- And MSE as the Loss function\n",
    "- Also add the following metrics: Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Fit (or train) your model\n",
    "- Use the X_train and y_train datasets for the training\n",
    "- Do 50 data iterations\n",
    "- Choose the batch size = 10\n",
    "- Also select a validation_split of 0.2\n",
    "- Save the result of the fit function in a variable called 'history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.3584 - mean_absolute_error: 1.1760 - val_loss: 0.1528 - val_mean_absolute_error: 0.3197\n",
      "Epoch 2/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1148 - mean_absolute_error: 0.2664 - val_loss: 0.0838 - val_mean_absolute_error: 0.2359\n",
      "Epoch 3/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0670 - mean_absolute_error: 0.2097 - val_loss: 0.0642 - val_mean_absolute_error: 0.2032\n",
      "Epoch 4/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0533 - mean_absolute_error: 0.1863 - val_loss: 0.0558 - val_mean_absolute_error: 0.1893\n",
      "Epoch 5/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0437 - mean_absolute_error: 0.1685 - val_loss: 0.0525 - val_mean_absolute_error: 0.1849\n",
      "Epoch 6/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0425 - mean_absolute_error: 0.1650 - val_loss: 0.0501 - val_mean_absolute_error: 0.1805\n",
      "Epoch 7/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0398 - mean_absolute_error: 0.1610 - val_loss: 0.0495 - val_mean_absolute_error: 0.1796\n",
      "Epoch 8/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0373 - mean_absolute_error: 0.1536 - val_loss: 0.0475 - val_mean_absolute_error: 0.1756\n",
      "Epoch 9/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0364 - mean_absolute_error: 0.1524 - val_loss: 0.0448 - val_mean_absolute_error: 0.1686\n",
      "Epoch 10/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0357 - mean_absolute_error: 0.1497 - val_loss: 0.0448 - val_mean_absolute_error: 0.1702\n",
      "Epoch 11/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0343 - mean_absolute_error: 0.1457 - val_loss: 0.0416 - val_mean_absolute_error: 0.1638\n",
      "Epoch 12/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0328 - mean_absolute_error: 0.1445 - val_loss: 0.0454 - val_mean_absolute_error: 0.1716\n",
      "Epoch 13/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0328 - mean_absolute_error: 0.1461 - val_loss: 0.0415 - val_mean_absolute_error: 0.1643\n",
      "Epoch 14/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0309 - mean_absolute_error: 0.1395 - val_loss: 0.0423 - val_mean_absolute_error: 0.1653\n",
      "Epoch 15/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0309 - mean_absolute_error: 0.1405 - val_loss: 0.0411 - val_mean_absolute_error: 0.1627\n",
      "Epoch 16/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0285 - mean_absolute_error: 0.1330 - val_loss: 0.0416 - val_mean_absolute_error: 0.1624\n",
      "Epoch 17/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0305 - mean_absolute_error: 0.1383 - val_loss: 0.0413 - val_mean_absolute_error: 0.1634\n",
      "Epoch 18/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0276 - mean_absolute_error: 0.1307 - val_loss: 0.0431 - val_mean_absolute_error: 0.1643\n",
      "Epoch 19/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0303 - mean_absolute_error: 0.1378 - val_loss: 0.0453 - val_mean_absolute_error: 0.1674\n",
      "Epoch 20/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0264 - mean_absolute_error: 0.1308 - val_loss: 0.0424 - val_mean_absolute_error: 0.1635\n",
      "Epoch 21/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0287 - mean_absolute_error: 0.1344 - val_loss: 0.0419 - val_mean_absolute_error: 0.1622\n",
      "Epoch 22/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0256 - mean_absolute_error: 0.1263 - val_loss: 0.0400 - val_mean_absolute_error: 0.1592\n",
      "Epoch 23/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0261 - mean_absolute_error: 0.1293 - val_loss: 0.0444 - val_mean_absolute_error: 0.1701\n",
      "Epoch 24/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0248 - mean_absolute_error: 0.1262 - val_loss: 0.0404 - val_mean_absolute_error: 0.1603\n",
      "Epoch 25/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0280 - mean_absolute_error: 0.1342 - val_loss: 0.0385 - val_mean_absolute_error: 0.1558\n",
      "Epoch 26/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0244 - mean_absolute_error: 0.1248 - val_loss: 0.0410 - val_mean_absolute_error: 0.1611\n",
      "Epoch 27/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0239 - mean_absolute_error: 0.1236 - val_loss: 0.0421 - val_mean_absolute_error: 0.1627\n",
      "Epoch 28/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0261 - mean_absolute_error: 0.1278 - val_loss: 0.0411 - val_mean_absolute_error: 0.1599\n",
      "Epoch 29/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0238 - mean_absolute_error: 0.1211 - val_loss: 0.0403 - val_mean_absolute_error: 0.1596\n",
      "Epoch 30/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0232 - mean_absolute_error: 0.1206 - val_loss: 0.0431 - val_mean_absolute_error: 0.1632\n",
      "Epoch 31/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0240 - mean_absolute_error: 0.1224 - val_loss: 0.0412 - val_mean_absolute_error: 0.1600\n",
      "Epoch 32/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0252 - mean_absolute_error: 0.1266 - val_loss: 0.0411 - val_mean_absolute_error: 0.1619\n",
      "Epoch 33/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0219 - mean_absolute_error: 0.1158 - val_loss: 0.0436 - val_mean_absolute_error: 0.1670\n",
      "Epoch 34/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0228 - mean_absolute_error: 0.1191 - val_loss: 0.0415 - val_mean_absolute_error: 0.1607\n",
      "Epoch 35/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0235 - mean_absolute_error: 0.1213 - val_loss: 0.0392 - val_mean_absolute_error: 0.1569\n",
      "Epoch 36/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0233 - mean_absolute_error: 0.1202 - val_loss: 0.0409 - val_mean_absolute_error: 0.1585\n",
      "Epoch 37/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0230 - mean_absolute_error: 0.1191 - val_loss: 0.0401 - val_mean_absolute_error: 0.1571\n",
      "Epoch 38/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0251 - mean_absolute_error: 0.1251 - val_loss: 0.0417 - val_mean_absolute_error: 0.1595\n",
      "Epoch 39/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0231 - mean_absolute_error: 0.1188 - val_loss: 0.0400 - val_mean_absolute_error: 0.1556\n",
      "Epoch 40/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0232 - mean_absolute_error: 0.1154 - val_loss: 0.0418 - val_mean_absolute_error: 0.1582\n",
      "Epoch 41/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0227 - mean_absolute_error: 0.1179 - val_loss: 0.0390 - val_mean_absolute_error: 0.1537\n",
      "Epoch 42/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0237 - mean_absolute_error: 0.1238 - val_loss: 0.0403 - val_mean_absolute_error: 0.1567\n",
      "Epoch 43/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0209 - mean_absolute_error: 0.1146 - val_loss: 0.0420 - val_mean_absolute_error: 0.1597\n",
      "Epoch 44/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0228 - mean_absolute_error: 0.1188 - val_loss: 0.0483 - val_mean_absolute_error: 0.1789\n",
      "Epoch 45/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0225 - mean_absolute_error: 0.1199 - val_loss: 0.0427 - val_mean_absolute_error: 0.1626\n",
      "Epoch 46/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0208 - mean_absolute_error: 0.1140 - val_loss: 0.0426 - val_mean_absolute_error: 0.1626\n",
      "Epoch 47/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0195 - mean_absolute_error: 0.1086 - val_loss: 0.0451 - val_mean_absolute_error: 0.1663\n",
      "Epoch 48/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0235 - mean_absolute_error: 0.1223 - val_loss: 0.0405 - val_mean_absolute_error: 0.1597\n",
      "Epoch 49/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0208 - mean_absolute_error: 0.1133 - val_loss: 0.0458 - val_mean_absolute_error: 0.1676\n",
      "Epoch 50/50\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0191 - mean_absolute_error: 0.1091 - val_loss: 0.0449 - val_mean_absolute_error: 0.1694\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "history = model.fit(X_train, y_train, batch_size=10, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. View your history variable:\n",
    "- Use Matplotlib.pyplot to show graphs of your model traning history\n",
    "- In one graph:\n",
    "   - Plot the Training Loss and the Validation Loss\n",
    "   - X Label = Epochs\n",
    "   - Y Label = Loss\n",
    "   - Title = Training and Validation Loss over Epochs\n",
    "- In a second graph:\n",
    "   - Plot the Training MAE and the Validation MAE\n",
    "   - X Label = Epochs\n",
    "   - Y Label = Mean Absolute Error (MAE)\n",
    "   - Title = Training and Validation MAE over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: title={'center': 'Training and Validation MAE over Epochs'}, xlabel='Epochs', ylabel='Mean Absolute Error (MAE)'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here\n",
    "df = pd.DataFrame(history.history)\n",
    "df.plot(y=['loss', 'val_loss'], title='Training and Validation Loss over Epochs', xlabel='Epochs', ylabel='Loss')\n",
    "df.plot(y=['mean_absolute_error', 'val_mean_absolute_error'], title='Training and Validation MAE over Epochs', xlabel='Epochs', ylabel='Mean Absolute Error (MAE)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Evaluate your model:\n",
    "- See the result of your loss function.\n",
    "- What can you deduct from there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0525 - mean_absolute_error: 0.1813 \n",
      "Loss: 0.0507708303630352\n",
      "MAE: 0.17930634319782257\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"MAE:\", mae)\n",
    "# The loss function in average is low, which means the model has good performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Use your model to make some predictions:\n",
    "- Make predictions of your X_test dataset\n",
    "- Print the each of the predictions and the actual value (which is in y_test)\n",
    "- How good was your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Prediction: 1.1655, Actual Value: 1.4277\n",
      "Prediction: 2.5097, Actual Value: 3.1174\n",
      "Prediction: 2.1502, Actual Value: 2.0378\n",
      "Prediction: 3.4664, Actual Value: 3.5485\n",
      "Prediction: 0.5188, Actual Value: 0.2490\n",
      "Prediction: 2.7890, Actual Value: 2.6277\n",
      "Prediction: 1.5745, Actual Value: 2.0574\n",
      "Prediction: 2.2046, Actual Value: 2.2483\n",
      "Prediction: 2.2946, Actual Value: 2.1947\n",
      "Prediction: 0.8723, Actual Value: 0.7582\n",
      "Prediction: 2.8253, Actual Value: 2.3709\n",
      "Prediction: 0.6146, Actual Value: 0.7664\n",
      "Prediction: 2.9506, Actual Value: 2.9527\n",
      "Prediction: 2.6896, Actual Value: 2.3433\n",
      "Prediction: 2.6582, Actual Value: 2.7718\n",
      "Prediction: 0.2652, Actual Value: 0.2879\n",
      "Prediction: 1.0766, Actual Value: 1.0183\n",
      "Prediction: 1.5099, Actual Value: 1.6294\n",
      "Prediction: 2.0965, Actual Value: 2.0744\n",
      "Prediction: 2.4242, Actual Value: 2.4238\n",
      "Prediction: 1.9831, Actual Value: 1.7562\n",
      "Prediction: 1.5987, Actual Value: 1.5663\n",
      "Prediction: 1.7460, Actual Value: 1.7062\n",
      "Prediction: 3.0610, Actual Value: 3.1614\n",
      "Prediction: 1.5757, Actual Value: 1.7334\n",
      "Prediction: 0.5655, Actual Value: 0.8420\n",
      "Prediction: 1.7641, Actual Value: 1.3792\n",
      "Prediction: 2.5760, Actual Value: 3.0270\n",
      "Prediction: 1.9213, Actual Value: 2.1920\n",
      "Prediction: 1.9460, Actual Value: 2.3158\n",
      "Prediction: 1.9483, Actual Value: 2.0681\n",
      "Prediction: 0.6868, Actual Value: 0.8691\n",
      "Prediction: 2.7861, Actual Value: 2.9001\n",
      "Prediction: 3.2662, Actual Value: 3.4686\n",
      "Prediction: 1.4847, Actual Value: 1.5674\n",
      "Prediction: 1.7122, Actual Value: 1.7947\n",
      "Prediction: 3.3740, Actual Value: 3.1813\n",
      "Prediction: 3.0424, Actual Value: 2.8974\n",
      "Prediction: 3.1594, Actual Value: 3.2449\n",
      "Prediction: 0.7042, Actual Value: 0.3578\n",
      "Prediction: 2.6560, Actual Value: 2.6524\n",
      "Prediction: 3.5027, Actual Value: 3.6810\n",
      "Prediction: 1.0949, Actual Value: 1.0364\n",
      "Prediction: 2.1728, Actual Value: 2.0172\n",
      "Prediction: 0.8059, Actual Value: 0.9634\n",
      "Prediction: 2.5193, Actual Value: 2.2395\n",
      "Prediction: 2.8409, Actual Value: 2.7360\n",
      "Prediction: 1.1025, Actual Value: 1.3619\n",
      "Prediction: 2.9775, Actual Value: 2.7030\n",
      "Prediction: 1.4231, Actual Value: 1.4419\n",
      "Prediction: 3.0355, Actual Value: 3.2195\n",
      "Prediction: 3.2191, Actual Value: 3.3391\n",
      "Prediction: 1.3928, Actual Value: 1.5562\n",
      "Prediction: 1.0344, Actual Value: 1.3424\n",
      "Prediction: 1.8165, Actual Value: 1.7562\n",
      "Prediction: 3.2164, Actual Value: 3.4218\n",
      "Prediction: 2.7536, Actual Value: 2.3170\n",
      "Prediction: 3.3228, Actual Value: 3.2866\n",
      "Prediction: 1.0024, Actual Value: 0.6847\n",
      "Prediction: 2.2229, Actual Value: 2.1371\n",
      "Prediction: 1.6434, Actual Value: 1.6885\n",
      "Prediction: 1.9875, Actual Value: 2.1962\n",
      "Prediction: 2.3334, Actual Value: 2.4778\n",
      "Prediction: 1.5678, Actual Value: 1.1979\n",
      "Prediction: 1.1047, Actual Value: 0.9882\n",
      "Prediction: 2.1553, Actual Value: 1.8589\n",
      "Prediction: 2.8533, Actual Value: 3.0407\n",
      "Prediction: 2.4892, Actual Value: 2.3741\n",
      "Prediction: 0.9887, Actual Value: 1.2213\n",
      "Prediction: 3.2032, Actual Value: 3.2742\n",
      "Prediction: 3.6382, Actual Value: 3.5452\n",
      "Prediction: 0.9139, Actual Value: 1.1019\n",
      "Prediction: 3.0528, Actual Value: 2.9688\n",
      "Prediction: 2.8722, Actual Value: 2.5762\n",
      "Prediction: 0.9430, Actual Value: 0.4541\n",
      "Prediction: 2.8050, Actual Value: 2.6742\n",
      "Prediction: 1.8215, Actual Value: 2.0600\n",
      "Prediction: 1.5313, Actual Value: 1.9831\n",
      "Prediction: 0.2451, Actual Value: 0.2124\n",
      "Prediction: 1.3817, Actual Value: 1.2794\n",
      "Prediction: 1.8573, Actual Value: 2.0297\n",
      "Prediction: 3.0393, Actual Value: 3.1892\n",
      "Prediction: 2.0481, Actual Value: 2.2565\n",
      "Prediction: 1.5610, Actual Value: 1.6364\n",
      "Prediction: 1.4694, Actual Value: 1.4016\n",
      "Prediction: 0.6595, Actual Value: 0.4436\n",
      "Prediction: 3.8181, Actual Value: 4.0000\n",
      "Prediction: 1.1857, Actual Value: 1.2712\n",
      "Prediction: 1.3938, Actual Value: 1.3026\n",
      "Prediction: 1.6742, Actual Value: 1.5970\n",
      "Prediction: 2.6881, Actual Value: 2.5354\n",
      "Prediction: 1.4020, Actual Value: 1.3829\n",
      "Prediction: 0.9449, Actual Value: 1.0385\n",
      "Prediction: 0.6465, Actual Value: 0.1307\n",
      "Prediction: 1.8795, Actual Value: 1.2738\n",
      "Prediction: 1.0712, Actual Value: 1.2715\n",
      "Prediction: 3.0535, Actual Value: 3.3251\n",
      "Prediction: 1.1068, Actual Value: 1.0707\n",
      "Prediction: 1.6245, Actual Value: 1.5743\n",
      "Prediction: 1.6000, Actual Value: 1.6337\n",
      "Prediction: 0.2419, Actual Value: 0.0000\n",
      "Prediction: 2.2524, Actual Value: 2.1221\n",
      "Prediction: 1.0992, Actual Value: 1.2912\n",
      "Prediction: 1.4666, Actual Value: 1.4547\n",
      "Prediction: 0.8454, Actual Value: 0.7743\n",
      "Prediction: 2.1781, Actual Value: 2.2931\n",
      "Prediction: 2.7912, Actual Value: 2.6828\n",
      "Prediction: 1.1801, Actual Value: 1.3155\n",
      "Prediction: 1.3900, Actual Value: 1.4694\n",
      "Prediction: 0.9665, Actual Value: 0.6132\n",
      "Prediction: 1.6675, Actual Value: 1.8820\n",
      "Prediction: 2.0071, Actual Value: 2.2689\n",
      "Prediction: 2.9199, Actual Value: 3.5452\n",
      "Prediction: 0.6451, Actual Value: 0.4258\n",
      "Prediction: 0.2689, Actual Value: 0.5404\n",
      "Prediction: 0.9859, Actual Value: 1.1111\n",
      "Prediction: 1.9930, Actual Value: 2.0354\n",
      "Prediction: 2.5925, Actual Value: 2.9955\n",
      "Prediction: 0.8413, Actual Value: 1.2179\n",
      "Prediction: 2.1597, Actual Value: 2.1270\n",
      "Prediction: 0.7691, Actual Value: 0.5298\n",
      "Prediction: 3.1432, Actual Value: 3.0169\n",
      "Prediction: 3.5757, Actual Value: 3.5921\n",
      "Prediction: 1.0038, Actual Value: 1.0583\n",
      "Prediction: 1.8653, Actual Value: 1.8426\n",
      "Prediction: 1.0320, Actual Value: 1.2546\n",
      "Prediction: 2.7013, Actual Value: 2.7949\n",
      "Prediction: 1.1096, Actual Value: 1.3479\n",
      "Prediction: 2.1186, Actual Value: 2.2270\n",
      "Prediction: 2.7028, Actual Value: 2.8135\n",
      "Prediction: 1.0866, Actual Value: 1.1056\n",
      "Prediction: 3.9392, Actual Value: 4.0000\n",
      "Prediction: 2.7030, Actual Value: 2.8982\n",
      "Prediction: 0.5680, Actual Value: 0.4240\n",
      "Prediction: 0.5502, Actual Value: 0.6915\n",
      "Prediction: 0.9280, Actual Value: 0.9300\n",
      "Prediction: 1.9714, Actual Value: 2.3308\n",
      "Prediction: 0.2590, Actual Value: 0.3376\n",
      "Prediction: 0.2617, Actual Value: 0.5022\n",
      "Prediction: 1.2103, Actual Value: 0.9893\n",
      "Prediction: 2.6128, Actual Value: 2.7459\n",
      "Prediction: 1.7502, Actual Value: 1.8042\n",
      "Prediction: 3.0685, Actual Value: 3.3433\n",
      "Prediction: 1.7370, Actual Value: 1.5731\n",
      "Prediction: 2.8729, Actual Value: 2.9640\n",
      "Prediction: 1.4245, Actual Value: 1.1936\n",
      "Prediction: 1.5512, Actual Value: 1.3493\n",
      "Prediction: 2.3765, Actual Value: 2.5202\n",
      "Prediction: 2.6697, Actual Value: 3.2538\n",
      "Prediction: 1.4964, Actual Value: 1.4634\n",
      "Prediction: 3.0670, Actual Value: 3.1620\n",
      "Prediction: 0.8882, Actual Value: 0.7696\n",
      "Prediction: 3.3495, Actual Value: 3.5037\n",
      "Prediction: 1.6278, Actual Value: 1.2688\n",
      "Prediction: 0.7440, Actual Value: 1.1708\n",
      "Prediction: 0.4565, Actual Value: 0.4713\n",
      "Prediction: 1.2365, Actual Value: 1.3979\n",
      "Prediction: 0.7756, Actual Value: 0.9690\n",
      "Prediction: 1.5111, Actual Value: 1.5113\n",
      "Prediction: 0.5925, Actual Value: 0.5242\n",
      "Prediction: 1.4153, Actual Value: 1.4914\n",
      "Prediction: 1.2587, Actual Value: 1.5194\n",
      "Prediction: 1.0402, Actual Value: 0.7133\n",
      "Prediction: 0.8382, Actual Value: 0.4006\n",
      "Prediction: 0.0422, Actual Value: 0.5814\n",
      "Prediction: 0.4543, Actual Value: 0.4051\n",
      "Prediction: 0.7235, Actual Value: 1.1544\n",
      "Prediction: 1.3866, Actual Value: 1.1502\n",
      "Prediction: 2.2090, Actual Value: 2.4579\n",
      "Prediction: 3.5050, Actual Value: 3.4555\n",
      "Prediction: 1.7345, Actual Value: 1.4414\n",
      "Prediction: 3.4917, Actual Value: 3.2880\n",
      "Prediction: 2.8939, Actual Value: 3.0439\n",
      "Prediction: 1.1710, Actual Value: 0.1898\n",
      "Prediction: 1.3739, Actual Value: 1.1658\n",
      "Prediction: 2.6637, Actual Value: 2.5654\n",
      "Prediction: 2.2459, Actual Value: 2.2347\n",
      "Prediction: 1.0931, Actual Value: 1.2587\n",
      "Prediction: 2.2360, Actual Value: 2.1167\n",
      "Prediction: 1.4408, Actual Value: 1.6747\n",
      "Prediction: 2.7736, Actual Value: 2.8211\n",
      "Prediction: 1.7337, Actual Value: 1.9961\n",
      "Prediction: 0.8580, Actual Value: 0.8091\n",
      "Prediction: 2.9481, Actual Value: 2.5466\n",
      "Prediction: 3.0532, Actual Value: 2.7350\n",
      "Prediction: 2.9373, Actual Value: 2.7536\n",
      "Prediction: 3.1388, Actual Value: 3.2839\n",
      "Prediction: 1.7080, Actual Value: 1.2878\n",
      "Prediction: 0.3200, Actual Value: 0.3107\n",
      "Prediction: 1.9357, Actual Value: 1.8315\n",
      "Prediction: 1.2890, Actual Value: 0.9116\n",
      "Prediction: 2.1071, Actual Value: 2.1097\n",
      "Prediction: 1.2512, Actual Value: 1.0445\n",
      "Prediction: 0.5362, Actual Value: 0.9655\n",
      "Prediction: 1.6453, Actual Value: 1.5504\n",
      "Prediction: 1.3924, Actual Value: 1.2238\n",
      "Prediction: 1.7802, Actual Value: 1.8934\n",
      "Prediction: 2.2470, Actual Value: 2.2954\n",
      "Prediction: 2.5625, Actual Value: 2.8393\n",
      "Prediction: 2.2147, Actual Value: 2.2518\n",
      "Prediction: 1.5655, Actual Value: 1.4053\n",
      "Prediction: 2.9229, Actual Value: 3.0321\n",
      "Prediction: 0.7564, Actual Value: 0.5694\n",
      "Prediction: 2.4279, Actual Value: 2.2201\n",
      "Prediction: 2.9207, Actual Value: 2.7374\n",
      "Prediction: 1.9691, Actual Value: 2.3408\n",
      "Prediction: 0.5862, Actual Value: 0.5602\n",
      "Prediction: 2.8737, Actual Value: 2.9139\n",
      "Prediction: 2.0516, Actual Value: 2.2595\n",
      "Prediction: 1.6674, Actual Value: 1.7420\n",
      "Prediction: 1.5280, Actual Value: 1.2659\n",
      "Prediction: 0.9707, Actual Value: 1.2987\n",
      "Prediction: 2.1034, Actual Value: 1.9138\n",
      "Prediction: 2.6238, Actual Value: 2.7808\n",
      "Prediction: 2.7717, Actual Value: 2.5535\n",
      "Prediction: 1.0118, Actual Value: 0.8923\n",
      "Prediction: 2.8912, Actual Value: 2.6697\n",
      "Prediction: 3.1873, Actual Value: 3.0885\n",
      "Prediction: 1.7684, Actual Value: 1.5636\n",
      "Prediction: 2.8116, Actual Value: 2.8757\n",
      "Prediction: 0.7829, Actual Value: 1.1845\n",
      "Prediction: 2.4324, Actual Value: 2.5842\n",
      "Prediction: 3.3985, Actual Value: 3.8128\n",
      "Prediction: 0.3348, Actual Value: 0.6729\n",
      "Prediction: 1.5710, Actual Value: 1.9559\n",
      "Prediction: 1.6983, Actual Value: 1.7733\n",
      "Prediction: 3.1006, Actual Value: 3.0554\n",
      "Prediction: 2.6024, Actual Value: 2.7185\n",
      "Prediction: 1.3736, Actual Value: 1.2705\n",
      "Prediction: 1.9433, Actual Value: 1.9880\n",
      "Prediction: 1.3866, Actual Value: 1.7209\n",
      "Prediction: 1.7203, Actual Value: 2.1106\n",
      "Prediction: 2.7777, Actual Value: 2.8816\n",
      "Prediction: 1.0391, Actual Value: 0.9708\n",
      "Prediction: 2.8763, Actual Value: 2.9729\n",
      "Prediction: 2.5621, Actual Value: 2.5210\n",
      "Prediction: 0.6196, Actual Value: 0.5152\n",
      "Prediction: 1.0229, Actual Value: 1.0086\n",
      "Prediction: 2.6549, Actual Value: 2.7913\n",
      "Prediction: 1.0152, Actual Value: 0.5868\n",
      "Prediction: 3.5889, Actual Value: 3.6663\n",
      "Prediction: 1.0332, Actual Value: 0.9031\n",
      "Prediction: 0.7585, Actual Value: 0.7046\n",
      "Prediction: 2.1535, Actual Value: 2.3983\n",
      "Prediction: 1.4165, Actual Value: 1.8178\n",
      "Prediction: 0.6201, Actual Value: 0.7996\n",
      "Prediction: 2.3160, Actual Value: 2.1274\n",
      "Prediction: 1.0822, Actual Value: 1.1768\n",
      "Prediction: 0.5414, Actual Value: 0.4291\n",
      "Prediction: 1.6258, Actual Value: 2.0469\n",
      "Prediction: 1.1038, Actual Value: 1.0862\n",
      "Prediction: 2.8665, Actual Value: 2.9869\n",
      "Prediction: 2.4733, Actual Value: 2.4740\n",
      "Prediction: 1.8085, Actual Value: 1.5819\n",
      "Prediction: 2.5235, Actual Value: 2.3278\n",
      "Prediction: 1.8343, Actual Value: 1.5253\n",
      "Prediction: 1.4157, Actual Value: 1.6831\n",
      "Prediction: 2.3272, Actual Value: 2.4610\n",
      "Prediction: 0.4092, Actual Value: 0.0278\n",
      "Prediction: 1.4702, Actual Value: 1.5364\n",
      "Prediction: 1.9213, Actual Value: 2.0999\n",
      "Prediction: 0.4868, Actual Value: 0.3751\n",
      "Prediction: 1.5054, Actual Value: 1.2946\n",
      "Prediction: 0.6456, Actual Value: 0.7697\n",
      "Prediction: 1.8138, Actual Value: 1.8214\n",
      "Prediction: 3.1769, Actual Value: 3.4983\n",
      "Prediction: 2.4233, Actual Value: 2.3781\n",
      "Prediction: 0.8032, Actual Value: 0.5185\n",
      "Prediction: 2.5893, Actual Value: 2.7549\n",
      "Prediction: 2.1011, Actual Value: 2.2070\n",
      "Prediction: 1.0842, Actual Value: 1.1416\n",
      "Prediction: 3.0269, Actual Value: 3.1376\n",
      "Prediction: 1.3312, Actual Value: 1.3638\n",
      "Prediction: 1.6418, Actual Value: 1.6885\n",
      "Prediction: 1.2553, Actual Value: 1.3503\n",
      "Prediction: 2.1730, Actual Value: 2.0625\n",
      "Prediction: 1.0022, Actual Value: 1.0226\n",
      "Prediction: 1.5679, Actual Value: 1.7923\n",
      "Prediction: 0.3048, Actual Value: 0.0000\n",
      "Prediction: 2.7271, Actual Value: 2.8729\n",
      "Prediction: 3.3621, Actual Value: 3.0887\n",
      "Prediction: 2.6010, Actual Value: 2.9375\n",
      "Prediction: 1.7795, Actual Value: 1.9901\n",
      "Prediction: 1.4822, Actual Value: 1.5165\n",
      "Prediction: 3.2143, Actual Value: 3.3009\n",
      "Prediction: 2.2277, Actual Value: 2.1727\n",
      "Prediction: 1.2553, Actual Value: 1.1194\n",
      "Prediction: 1.4106, Actual Value: 1.5718\n",
      "Prediction: 0.8073, Actual Value: 0.4959\n",
      "Prediction: 0.4633, Actual Value: 0.1550\n",
      "Prediction: 3.5934, Actual Value: 3.8650\n",
      "Prediction: 1.7229, Actual Value: 1.8044\n",
      "Prediction: 2.5335, Actual Value: 2.7842\n",
      "Prediction: 1.2650, Actual Value: 1.2059\n",
      "Prediction: 2.5432, Actual Value: 2.8236\n",
      "Prediction: 1.2602, Actual Value: 0.8952\n",
      "Prediction: 2.9118, Actual Value: 3.1123\n",
      "Prediction: 1.6664, Actual Value: 1.8154\n",
      "Prediction: 0.2350, Actual Value: 0.1006\n",
      "Prediction: 2.9325, Actual Value: 3.0655\n",
      "Prediction: 2.6463, Actual Value: 2.5539\n",
      "Prediction: 1.7602, Actual Value: 1.9864\n",
      "Prediction: 1.0748, Actual Value: 0.9941\n",
      "Prediction: 3.1438, Actual Value: 3.3501\n",
      "Prediction: 1.8776, Actual Value: 2.3117\n",
      "Prediction: 2.6392, Actual Value: 2.5042\n",
      "Prediction: 0.5864, Actual Value: 0.7635\n",
      "Prediction: 2.3654, Actual Value: 2.7122\n",
      "Prediction: 0.7545, Actual Value: 0.9253\n",
      "Prediction: 1.7447, Actual Value: 1.8498\n",
      "Prediction: 1.3796, Actual Value: 1.3971\n",
      "Prediction: 1.5262, Actual Value: 1.8890\n",
      "Prediction: 0.7433, Actual Value: 0.9000\n",
      "Prediction: 0.9829, Actual Value: 1.0277\n",
      "Prediction: 0.8827, Actual Value: 0.7017\n",
      "Prediction: 1.2561, Actual Value: 1.4120\n",
      "Prediction: 2.8470, Actual Value: 2.9774\n",
      "Prediction: 2.3496, Actual Value: 2.2104\n",
      "Prediction: 2.4008, Actual Value: 2.4387\n",
      "Prediction: 1.7864, Actual Value: 1.9213\n",
      "Prediction: 1.9257, Actual Value: 1.7340\n",
      "Prediction: 1.1923, Actual Value: 1.5893\n",
      "Prediction: 1.7982, Actual Value: 1.5034\n",
      "Prediction: 2.1796, Actual Value: 2.2220\n",
      "Prediction: 3.1613, Actual Value: 3.1062\n",
      "Prediction: 1.5093, Actual Value: 1.5421\n",
      "Prediction: 0.9196, Actual Value: 1.2523\n",
      "Prediction: 1.0368, Actual Value: 1.7105\n",
      "Prediction: 2.5630, Actual Value: 2.6936\n",
      "Prediction: 1.0109, Actual Value: 0.9657\n",
      "Prediction: 1.7750, Actual Value: 1.8683\n",
      "Prediction: 2.8594, Actual Value: 2.8069\n",
      "Prediction: 2.2272, Actual Value: 2.3428\n",
      "Prediction: 2.5779, Actual Value: 2.5837\n",
      "Prediction: 3.2176, Actual Value: 3.2486\n",
      "Prediction: 2.7400, Actual Value: 2.5845\n",
      "Prediction: 2.6295, Actual Value: 2.5876\n",
      "Prediction: 1.3338, Actual Value: 1.5660\n",
      "Prediction: 0.2485, Actual Value: 0.8670\n",
      "Prediction: 1.9810, Actual Value: 1.8476\n",
      "Prediction: 1.2844, Actual Value: 1.1221\n",
      "Prediction: 1.5661, Actual Value: 1.7182\n",
      "Prediction: 1.5162, Actual Value: 1.1645\n",
      "Prediction: 1.5482, Actual Value: 1.6833\n",
      "Prediction: 1.9170, Actual Value: 2.0186\n",
      "Prediction: 2.5209, Actual Value: 2.5125\n",
      "Prediction: 3.0962, Actual Value: 3.1690\n",
      "Prediction: 3.4581, Actual Value: 3.0526\n",
      "Prediction: 1.0569, Actual Value: 1.1529\n",
      "Prediction: 3.2646, Actual Value: 3.2583\n",
      "Prediction: 3.2841, Actual Value: 3.3637\n",
      "Prediction: 2.2255, Actual Value: 2.4041\n",
      "Prediction: 2.1230, Actual Value: 2.1377\n",
      "Prediction: 2.4127, Actual Value: 2.4276\n",
      "Prediction: 2.0779, Actual Value: 1.8100\n",
      "Prediction: 1.8060, Actual Value: 1.7560\n",
      "Prediction: 2.2710, Actual Value: 2.0531\n",
      "Prediction: 1.1711, Actual Value: 1.6015\n",
      "Prediction: 0.6821, Actual Value: 0.4696\n",
      "Prediction: 2.0299, Actual Value: 2.0046\n",
      "Prediction: 2.5450, Actual Value: 2.2888\n",
      "Prediction: 3.1937, Actual Value: 3.0917\n",
      "Prediction: 2.1795, Actual Value: 2.1172\n",
      "Prediction: 1.1304, Actual Value: 1.3476\n",
      "Prediction: 1.7002, Actual Value: 1.5304\n",
      "Prediction: 2.7341, Actual Value: 2.7344\n",
      "Prediction: 3.5438, Actual Value: 3.8302\n",
      "Prediction: 2.5532, Actual Value: 2.6394\n",
      "Prediction: 2.0621, Actual Value: 2.4714\n",
      "Prediction: 2.0971, Actual Value: 2.2286\n",
      "Prediction: 0.4770, Actual Value: 0.6566\n",
      "Prediction: 2.1451, Actual Value: 2.2598\n",
      "Prediction: 2.4423, Actual Value: 2.3328\n",
      "Prediction: 2.8359, Actual Value: 3.0235\n",
      "Prediction: 2.4017, Actual Value: 2.9647\n",
      "Prediction: 1.0994, Actual Value: 0.9836\n",
      "Prediction: 1.4684, Actual Value: 1.4655\n",
      "Prediction: 2.4009, Actual Value: 2.2689\n",
      "Prediction: 3.3247, Actual Value: 3.5729\n",
      "Prediction: 0.9080, Actual Value: 0.6823\n",
      "Prediction: 1.8201, Actual Value: 1.9847\n",
      "Prediction: 2.3739, Actual Value: 2.5197\n",
      "Prediction: 0.5139, Actual Value: 0.5496\n",
      "Prediction: 2.7324, Actual Value: 2.9869\n",
      "Prediction: 2.7163, Actual Value: 2.6405\n",
      "Prediction: 1.2673, Actual Value: 1.4773\n",
      "Prediction: 2.0054, Actual Value: 1.9396\n",
      "Prediction: 1.7840, Actual Value: 1.8298\n",
      "Prediction: 2.4878, Actual Value: 2.6696\n",
      "Prediction: 1.6526, Actual Value: 1.5961\n",
      "Prediction: 2.3201, Actual Value: 2.2790\n",
      "Prediction: 0.8214, Actual Value: 1.1378\n",
      "Prediction: 0.9878, Actual Value: 0.6768\n",
      "Prediction: 2.3579, Actual Value: 2.5040\n",
      "Prediction: 2.1263, Actual Value: 2.1226\n",
      "Prediction: 2.6692, Actual Value: 2.9779\n",
      "Prediction: 2.2928, Actual Value: 2.8276\n",
      "Prediction: 2.9162, Actual Value: 2.9422\n",
      "Prediction: 2.2669, Actual Value: 2.3100\n",
      "Prediction: 2.7983, Actual Value: 2.7779\n",
      "Prediction: 2.4376, Actual Value: 2.5244\n",
      "Prediction: 2.1510, Actual Value: 2.1353\n",
      "Prediction: 2.4454, Actual Value: 2.2242\n",
      "Prediction: 2.9729, Actual Value: 3.2389\n",
      "Prediction: 2.0347, Actual Value: 2.2303\n",
      "Prediction: 2.8833, Actual Value: 3.0608\n",
      "Prediction: 0.7704, Actual Value: 1.3636\n",
      "Prediction: 3.3251, Actual Value: 3.6035\n",
      "Prediction: 2.0870, Actual Value: 2.1673\n",
      "Prediction: 2.8782, Actual Value: 3.3239\n",
      "Prediction: 1.9037, Actual Value: 2.0166\n",
      "Prediction: 1.5211, Actual Value: 1.3620\n",
      "Prediction: 1.8975, Actual Value: 2.1156\n",
      "Prediction: 2.4820, Actual Value: 2.4653\n",
      "Prediction: 3.0833, Actual Value: 3.0605\n",
      "Prediction: 1.8964, Actual Value: 1.9915\n",
      "Prediction: 0.4089, Actual Value: 0.2649\n",
      "Prediction: 3.3008, Actual Value: 3.6457\n",
      "Prediction: 2.8356, Actual Value: 2.5172\n",
      "Prediction: 1.4938, Actual Value: 1.5487\n",
      "Prediction: 1.4281, Actual Value: 1.5996\n",
      "Prediction: 1.6426, Actual Value: 1.5539\n",
      "Prediction: 2.4172, Actual Value: 2.5959\n",
      "Prediction: 2.0532, Actual Value: 1.8791\n",
      "Prediction: 1.1271, Actual Value: 1.5067\n",
      "Prediction: 1.2513, Actual Value: 1.1049\n",
      "Prediction: 0.5794, Actual Value: 0.3414\n",
      "Prediction: 2.3408, Actual Value: 2.4053\n",
      "Prediction: 1.9386, Actual Value: 1.6848\n",
      "Prediction: 2.9206, Actual Value: 2.9935\n",
      "Prediction: 3.0130, Actual Value: 3.4154\n",
      "Prediction: 1.4827, Actual Value: 1.7097\n",
      "Prediction: 1.7675, Actual Value: 1.5387\n",
      "Prediction: 2.8921, Actual Value: 2.9214\n",
      "Prediction: 2.3917, Actual Value: 2.6052\n",
      "Prediction: 2.1541, Actual Value: 2.4400\n",
      "Prediction: 1.1337, Actual Value: 1.4290\n",
      "Prediction: 3.1219, Actual Value: 3.3721\n",
      "Prediction: 2.2810, Actual Value: 2.1516\n",
      "Prediction: 1.5373, Actual Value: 1.5951\n",
      "Prediction: 1.3181, Actual Value: 1.4146\n",
      "Prediction: 3.0142, Actual Value: 3.1289\n",
      "Prediction: 2.8155, Actual Value: 2.5534\n",
      "Prediction: 2.9634, Actual Value: 3.1296\n",
      "Prediction: 2.8880, Actual Value: 2.9820\n",
      "Prediction: 2.9460, Actual Value: 2.9818\n",
      "Prediction: 1.0489, Actual Value: 1.2011\n",
      "Prediction: 2.5611, Actual Value: 2.7914\n",
      "Prediction: 0.7429, Actual Value: 0.1530\n",
      "Prediction: 0.7668, Actual Value: 0.6194\n",
      "Prediction: 2.1368, Actual Value: 2.2813\n",
      "Prediction: 0.6408, Actual Value: 0.4937\n",
      "Prediction: 2.8245, Actual Value: 2.9665\n",
      "Prediction: 2.0167, Actual Value: 1.9895\n",
      "Prediction: 3.6632, Actual Value: 3.5433\n",
      "Prediction: 1.8379, Actual Value: 1.9706\n",
      "Prediction: 1.0098, Actual Value: 1.3441\n",
      "Prediction: 0.8215, Actual Value: 1.0270\n",
      "Prediction: 0.5789, Actual Value: 0.4278\n",
      "Prediction: 2.0449, Actual Value: 1.9747\n",
      "Prediction: 0.9014, Actual Value: 1.2422\n",
      "Prediction: 0.7744, Actual Value: 0.3844\n",
      "Prediction: 0.7499, Actual Value: 0.4475\n",
      "Prediction: 1.3252, Actual Value: 1.7291\n",
      "Prediction: 2.2145, Actual Value: 2.1364\n",
      "Prediction: 0.1923, Actual Value: 0.2110\n",
      "Prediction: 0.1591, Actual Value: 0.3310\n",
      "Prediction: 2.8151, Actual Value: 2.8889\n",
      "Prediction: 3.1407, Actual Value: 3.2704\n",
      "Prediction: 2.7329, Actual Value: 2.5001\n",
      "Prediction: 1.3041, Actual Value: 1.2129\n",
      "Prediction: 1.9303, Actual Value: 2.0093\n",
      "Prediction: 1.2872, Actual Value: 1.5438\n",
      "Prediction: 1.1669, Actual Value: 1.4382\n",
      "Prediction: 1.8438, Actual Value: 1.5624\n",
      "Prediction: 2.0802, Actual Value: 2.1749\n",
      "Prediction: 2.4781, Actual Value: 2.3325\n",
      "Prediction: 2.3827, Actual Value: 2.7780\n",
      "Prediction: 0.8235, Actual Value: 0.8635\n",
      "Mean Squared Error (MSE): 0.050770828663384095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eryke\\AppData\\Local\\Temp\\ipykernel_21552\\1436540352.py:6: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(f\"Prediction: {predictions[i][0]:.4f}, Actual Value: {float(y_test_array[i]):.4f}\")\n"
     ]
    }
   ],
   "source": [
    "y_test_array = np.array(y_test)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    print(f\"Prediction: {predictions[i][0]:.4f}, Actual Value: {float(y_test_array[i]):.4f}\")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calcular el MSE\n",
    "mse = mean_squared_error(y_test_array, predictions)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "# It has a low MSE which means the model is good at predicting GPA scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Compete against this model:\n",
    "- Create two more different models to compete with this model\n",
    "- Here are a few ideas of things you can change:\n",
    "   - During Dataset data engineering:\n",
    "      - You can remove features that you think do not help in the training and prediction \n",
    "      - Feature Scaling: Ensure all features are on a similar scale (as you already did with StandardScaler)\n",
    "   - During Model Definition:\n",
    "      - You can change the Model Architecture (change the type or number of layers or the number of units)\n",
    "      - You can add dropout layers to prevent overfitting\n",
    "   - During Model Compile:\n",
    "      - You can try other optimizer when compiling your model, here some optimizer samples: Adam, RMSprop, or Adagrad.\n",
    "      - Try another Loss Function\n",
    "   - During Model Training:\n",
    "      - Encrease the number of Epochs\n",
    "      - Adjust the size of your batch\n",
    "- Explain in a Markdown cell which changes are you implementing\n",
    "- Show the comparison of your model versus the original model\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2:\n",
    "- Changes:\n",
    "   - Dataset Data Engineering\n",
    "   - Model Definition\n",
    "   - Model Compile\n",
    "   - Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eryke\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 1.3162 - mean_absolute_error: 0.8303 - val_loss: 0.1182 - val_mean_absolute_error: 0.2778\n",
      "Epoch 2/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0787 - mean_absolute_error: 0.2241 - val_loss: 0.0841 - val_mean_absolute_error: 0.2363\n",
      "Epoch 3/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0577 - mean_absolute_error: 0.1896 - val_loss: 0.0650 - val_mean_absolute_error: 0.2065\n",
      "Epoch 4/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0552 - mean_absolute_error: 0.1883 - val_loss: 0.0537 - val_mean_absolute_error: 0.1838\n",
      "Epoch 5/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0499 - mean_absolute_error: 0.1770 - val_loss: 0.0566 - val_mean_absolute_error: 0.1912\n",
      "Epoch 6/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0463 - mean_absolute_error: 0.1718 - val_loss: 0.0703 - val_mean_absolute_error: 0.2166\n",
      "Epoch 7/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0438 - mean_absolute_error: 0.1644 - val_loss: 0.0517 - val_mean_absolute_error: 0.1774\n",
      "Epoch 8/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0420 - mean_absolute_error: 0.1624 - val_loss: 0.0781 - val_mean_absolute_error: 0.2258\n",
      "Epoch 9/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0430 - mean_absolute_error: 0.1663 - val_loss: 0.0651 - val_mean_absolute_error: 0.2033\n",
      "Epoch 10/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0409 - mean_absolute_error: 0.1577 - val_loss: 0.0502 - val_mean_absolute_error: 0.1739\n",
      "Epoch 11/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0416 - mean_absolute_error: 0.1625 - val_loss: 0.0525 - val_mean_absolute_error: 0.1804\n",
      "Epoch 12/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0406 - mean_absolute_error: 0.1597 - val_loss: 0.0664 - val_mean_absolute_error: 0.2098\n",
      "Epoch 13/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0374 - mean_absolute_error: 0.1484 - val_loss: 0.0496 - val_mean_absolute_error: 0.1735\n",
      "Epoch 14/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0359 - mean_absolute_error: 0.1467 - val_loss: 0.0552 - val_mean_absolute_error: 0.1833\n",
      "Epoch 15/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0347 - mean_absolute_error: 0.1471 - val_loss: 0.0509 - val_mean_absolute_error: 0.1798\n",
      "Epoch 16/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0333 - mean_absolute_error: 0.1440 - val_loss: 0.0507 - val_mean_absolute_error: 0.1772\n",
      "Epoch 17/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0336 - mean_absolute_error: 0.1454 - val_loss: 0.0536 - val_mean_absolute_error: 0.1826\n",
      "Epoch 18/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0363 - mean_absolute_error: 0.1489 - val_loss: 0.0490 - val_mean_absolute_error: 0.1740\n",
      "Epoch 19/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0336 - mean_absolute_error: 0.1465 - val_loss: 0.0516 - val_mean_absolute_error: 0.1783\n",
      "Epoch 20/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0353 - mean_absolute_error: 0.1484 - val_loss: 0.0462 - val_mean_absolute_error: 0.1671\n",
      "Epoch 21/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0312 - mean_absolute_error: 0.1374 - val_loss: 0.0558 - val_mean_absolute_error: 0.1883\n",
      "Epoch 22/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0317 - mean_absolute_error: 0.1399 - val_loss: 0.0521 - val_mean_absolute_error: 0.1766\n",
      "Epoch 23/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0323 - mean_absolute_error: 0.1423 - val_loss: 0.0519 - val_mean_absolute_error: 0.1787\n",
      "Epoch 24/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0297 - mean_absolute_error: 0.1349 - val_loss: 0.0506 - val_mean_absolute_error: 0.1756\n",
      "Epoch 25/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0300 - mean_absolute_error: 0.1371 - val_loss: 0.0517 - val_mean_absolute_error: 0.1795\n",
      "Epoch 26/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0311 - mean_absolute_error: 0.1360 - val_loss: 0.0480 - val_mean_absolute_error: 0.1700\n",
      "Epoch 27/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0273 - mean_absolute_error: 0.1293 - val_loss: 0.0515 - val_mean_absolute_error: 0.1754\n",
      "Epoch 28/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0302 - mean_absolute_error: 0.1363 - val_loss: 0.0637 - val_mean_absolute_error: 0.2062\n",
      "Epoch 29/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0308 - mean_absolute_error: 0.1403 - val_loss: 0.0504 - val_mean_absolute_error: 0.1779\n",
      "Epoch 30/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0267 - mean_absolute_error: 0.1258 - val_loss: 0.0734 - val_mean_absolute_error: 0.2234\n",
      "Epoch 31/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0293 - mean_absolute_error: 0.1314 - val_loss: 0.0480 - val_mean_absolute_error: 0.1707\n",
      "Epoch 32/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0275 - mean_absolute_error: 0.1314 - val_loss: 0.0546 - val_mean_absolute_error: 0.1840\n",
      "Epoch 33/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0280 - mean_absolute_error: 0.1330 - val_loss: 0.0499 - val_mean_absolute_error: 0.1750\n",
      "Epoch 34/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0272 - mean_absolute_error: 0.1284 - val_loss: 0.0778 - val_mean_absolute_error: 0.2250\n",
      "Epoch 35/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0274 - mean_absolute_error: 0.1305 - val_loss: 0.0551 - val_mean_absolute_error: 0.1872\n",
      "Epoch 36/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0247 - mean_absolute_error: 0.1222 - val_loss: 0.0504 - val_mean_absolute_error: 0.1775\n",
      "Epoch 37/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0245 - mean_absolute_error: 0.1262 - val_loss: 0.0517 - val_mean_absolute_error: 0.1789\n",
      "Epoch 38/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0275 - mean_absolute_error: 0.1305 - val_loss: 0.0643 - val_mean_absolute_error: 0.2021\n",
      "Epoch 39/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0258 - mean_absolute_error: 0.1257 - val_loss: 0.0511 - val_mean_absolute_error: 0.1742\n",
      "Epoch 40/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0276 - mean_absolute_error: 0.1293 - val_loss: 0.0665 - val_mean_absolute_error: 0.2080\n",
      "Epoch 41/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0275 - mean_absolute_error: 0.1304 - val_loss: 0.0473 - val_mean_absolute_error: 0.1715\n",
      "Epoch 42/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0248 - mean_absolute_error: 0.1222 - val_loss: 0.0502 - val_mean_absolute_error: 0.1763\n",
      "Epoch 43/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0233 - mean_absolute_error: 0.1210 - val_loss: 0.0522 - val_mean_absolute_error: 0.1812\n",
      "Epoch 44/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0239 - mean_absolute_error: 0.1220 - val_loss: 0.0522 - val_mean_absolute_error: 0.1819\n",
      "Epoch 45/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0231 - mean_absolute_error: 0.1196 - val_loss: 0.0533 - val_mean_absolute_error: 0.1777\n",
      "Epoch 46/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0235 - mean_absolute_error: 0.1213 - val_loss: 0.0626 - val_mean_absolute_error: 0.2026\n",
      "Epoch 47/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0238 - mean_absolute_error: 0.1214 - val_loss: 0.0494 - val_mean_absolute_error: 0.1746\n",
      "Epoch 48/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0230 - mean_absolute_error: 0.1200 - val_loss: 0.0512 - val_mean_absolute_error: 0.1778\n",
      "Epoch 49/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0222 - mean_absolute_error: 0.1154 - val_loss: 0.0481 - val_mean_absolute_error: 0.1721\n",
      "Epoch 50/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0232 - mean_absolute_error: 0.1186 - val_loss: 0.0500 - val_mean_absolute_error: 0.1763\n",
      "Epoch 51/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0223 - mean_absolute_error: 0.1166 - val_loss: 0.0471 - val_mean_absolute_error: 0.1705\n",
      "Epoch 52/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0233 - mean_absolute_error: 0.1167 - val_loss: 0.0520 - val_mean_absolute_error: 0.1806\n",
      "Epoch 53/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0201 - mean_absolute_error: 0.1104 - val_loss: 0.0521 - val_mean_absolute_error: 0.1824\n",
      "Epoch 54/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0217 - mean_absolute_error: 0.1153 - val_loss: 0.0533 - val_mean_absolute_error: 0.1845\n",
      "Epoch 55/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0222 - mean_absolute_error: 0.1170 - val_loss: 0.0663 - val_mean_absolute_error: 0.2098\n",
      "Epoch 56/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0209 - mean_absolute_error: 0.1103 - val_loss: 0.0527 - val_mean_absolute_error: 0.1776\n",
      "Epoch 57/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0217 - mean_absolute_error: 0.1165 - val_loss: 0.0529 - val_mean_absolute_error: 0.1825\n",
      "Epoch 58/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0208 - mean_absolute_error: 0.1144 - val_loss: 0.0504 - val_mean_absolute_error: 0.1770\n",
      "Epoch 59/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0206 - mean_absolute_error: 0.1129 - val_loss: 0.0532 - val_mean_absolute_error: 0.1840\n",
      "Epoch 60/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0190 - mean_absolute_error: 0.1087 - val_loss: 0.0599 - val_mean_absolute_error: 0.1960\n",
      "Epoch 61/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0190 - mean_absolute_error: 0.1087 - val_loss: 0.0519 - val_mean_absolute_error: 0.1797\n",
      "Epoch 62/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0212 - mean_absolute_error: 0.1117 - val_loss: 0.0541 - val_mean_absolute_error: 0.1843\n",
      "Epoch 63/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0184 - mean_absolute_error: 0.1054 - val_loss: 0.0786 - val_mean_absolute_error: 0.2275\n",
      "Epoch 64/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0207 - mean_absolute_error: 0.1104 - val_loss: 0.0548 - val_mean_absolute_error: 0.1835\n",
      "Epoch 65/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0188 - mean_absolute_error: 0.1056 - val_loss: 0.0515 - val_mean_absolute_error: 0.1764\n",
      "Epoch 66/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0197 - mean_absolute_error: 0.1096 - val_loss: 0.0525 - val_mean_absolute_error: 0.1827\n",
      "Epoch 67/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0173 - mean_absolute_error: 0.1027 - val_loss: 0.0494 - val_mean_absolute_error: 0.1743\n",
      "Epoch 68/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0191 - mean_absolute_error: 0.1053 - val_loss: 0.0516 - val_mean_absolute_error: 0.1762\n",
      "Epoch 69/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0193 - mean_absolute_error: 0.1077 - val_loss: 0.0572 - val_mean_absolute_error: 0.1921\n",
      "Epoch 70/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0191 - mean_absolute_error: 0.1073 - val_loss: 0.0533 - val_mean_absolute_error: 0.1798\n",
      "Epoch 71/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0188 - mean_absolute_error: 0.1065 - val_loss: 0.0548 - val_mean_absolute_error: 0.1838\n",
      "Epoch 72/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0177 - mean_absolute_error: 0.1017 - val_loss: 0.0565 - val_mean_absolute_error: 0.1878\n",
      "Epoch 73/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0171 - mean_absolute_error: 0.1013 - val_loss: 0.0526 - val_mean_absolute_error: 0.1798\n",
      "Epoch 74/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0167 - mean_absolute_error: 0.1004 - val_loss: 0.0520 - val_mean_absolute_error: 0.1795\n",
      "Epoch 75/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0176 - mean_absolute_error: 0.1034 - val_loss: 0.0523 - val_mean_absolute_error: 0.1801\n",
      "Epoch 76/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0186 - mean_absolute_error: 0.1060 - val_loss: 0.0626 - val_mean_absolute_error: 0.1982\n",
      "Epoch 77/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0192 - mean_absolute_error: 0.1073 - val_loss: 0.0531 - val_mean_absolute_error: 0.1798\n",
      "Epoch 78/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0172 - mean_absolute_error: 0.1010 - val_loss: 0.0579 - val_mean_absolute_error: 0.1901\n",
      "Epoch 79/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0179 - mean_absolute_error: 0.1025 - val_loss: 0.0545 - val_mean_absolute_error: 0.1823\n",
      "Epoch 80/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0167 - mean_absolute_error: 0.0994 - val_loss: 0.0540 - val_mean_absolute_error: 0.1813\n",
      "Epoch 81/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0171 - mean_absolute_error: 0.1025 - val_loss: 0.0579 - val_mean_absolute_error: 0.1910\n",
      "Epoch 82/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0162 - mean_absolute_error: 0.0976 - val_loss: 0.0511 - val_mean_absolute_error: 0.1789\n",
      "Epoch 83/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0171 - mean_absolute_error: 0.1011 - val_loss: 0.0519 - val_mean_absolute_error: 0.1784\n",
      "Epoch 84/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0169 - mean_absolute_error: 0.0993 - val_loss: 0.0527 - val_mean_absolute_error: 0.1792\n",
      "Epoch 85/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0177 - mean_absolute_error: 0.0996 - val_loss: 0.0520 - val_mean_absolute_error: 0.1749\n",
      "Epoch 86/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0158 - mean_absolute_error: 0.0967 - val_loss: 0.0571 - val_mean_absolute_error: 0.1854\n",
      "Epoch 87/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0150 - mean_absolute_error: 0.0942 - val_loss: 0.0573 - val_mean_absolute_error: 0.1859\n",
      "Epoch 88/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0152 - mean_absolute_error: 0.0925 - val_loss: 0.0541 - val_mean_absolute_error: 0.1819\n",
      "Epoch 89/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0151 - mean_absolute_error: 0.0951 - val_loss: 0.0549 - val_mean_absolute_error: 0.1847\n",
      "Epoch 90/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0142 - mean_absolute_error: 0.0922 - val_loss: 0.0539 - val_mean_absolute_error: 0.1832\n",
      "Epoch 91/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0147 - mean_absolute_error: 0.0941 - val_loss: 0.0587 - val_mean_absolute_error: 0.1905\n",
      "Epoch 92/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0149 - mean_absolute_error: 0.0935 - val_loss: 0.0540 - val_mean_absolute_error: 0.1840\n",
      "Epoch 93/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0149 - mean_absolute_error: 0.0938 - val_loss: 0.0596 - val_mean_absolute_error: 0.1909\n",
      "Epoch 94/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0145 - mean_absolute_error: 0.0931 - val_loss: 0.0592 - val_mean_absolute_error: 0.1950\n",
      "Epoch 95/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0161 - mean_absolute_error: 0.0981 - val_loss: 0.0549 - val_mean_absolute_error: 0.1823\n",
      "Epoch 96/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0170 - mean_absolute_error: 0.1000 - val_loss: 0.0653 - val_mean_absolute_error: 0.2025\n",
      "Epoch 97/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0144 - mean_absolute_error: 0.0919 - val_loss: 0.0586 - val_mean_absolute_error: 0.1893\n",
      "Epoch 98/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0172 - mean_absolute_error: 0.1000 - val_loss: 0.0622 - val_mean_absolute_error: 0.1979\n",
      "Epoch 99/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0147 - mean_absolute_error: 0.0964 - val_loss: 0.0573 - val_mean_absolute_error: 0.1869\n",
      "Epoch 100/100\n",
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0141 - mean_absolute_error: 0.0894 - val_loss: 0.0714 - val_mean_absolute_error: 0.2145\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0848 - mean_absolute_error: 0.2320 \n",
      "Loss: 0.08100826293230057\n",
      "MAE: 0.2257031351327896\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Dataset Data Engineering\n",
    "dataset2 = data.drop(columns=['StudentID', 'Age', 'Gender', 'Ethnicity', 'Music', 'Sports'])\n",
    "X2 = dataset2.drop(\"GPA\", axis=1)\n",
    "y2 = dataset2[[\"GPA\"]]\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "scaler2 = StandardScaler()\n",
    "X_train2 = scaler2.fit_transform(X_train2)\n",
    "X_test2 = scaler2.transform(X_test2)\n",
    "\n",
    "# Model Definition\n",
    "model2 = Sequential([\n",
    "    Dense(128, input_dim=8, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Model Compile\n",
    "model2.compile(optimizer='RMSprop', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Model Training\n",
    "model2.fit(X_train2, y_train2, batch_size=15, epochs=100, validation_split=0.2)\n",
    "\n",
    "# Model Evaluation\n",
    "loss2, mse2 = model2.evaluate(X_test2, y_test2)\n",
    "print(\"Loss:\", loss2)\n",
    "print(\"MAE:\", mse2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3:\n",
    "- Changes:\n",
    "   - Dataset Data Engineering\n",
    "   - Model Definition\n",
    "   - Model Compile\n",
    "   - Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eryke\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.1328 - mean_absolute_error: 0.8198 - val_loss: 0.1493 - val_mean_absolute_error: 0.3111\n",
      "Epoch 2/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3118 - mean_absolute_error: 0.4337 - val_loss: 0.0918 - val_mean_absolute_error: 0.2440\n",
      "Epoch 3/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2451 - mean_absolute_error: 0.3869 - val_loss: 0.0846 - val_mean_absolute_error: 0.2384\n",
      "Epoch 4/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.2051 - mean_absolute_error: 0.3543 - val_loss: 0.1156 - val_mean_absolute_error: 0.2810\n",
      "Epoch 5/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1801 - mean_absolute_error: 0.3252 - val_loss: 0.0567 - val_mean_absolute_error: 0.1919\n",
      "Epoch 6/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1552 - mean_absolute_error: 0.3110 - val_loss: 0.0678 - val_mean_absolute_error: 0.2133\n",
      "Epoch 7/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1312 - mean_absolute_error: 0.2862 - val_loss: 0.0561 - val_mean_absolute_error: 0.1944\n",
      "Epoch 8/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1355 - mean_absolute_error: 0.2904 - val_loss: 0.0488 - val_mean_absolute_error: 0.1751\n",
      "Epoch 9/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1119 - mean_absolute_error: 0.2603 - val_loss: 0.0511 - val_mean_absolute_error: 0.1847\n",
      "Epoch 10/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1095 - mean_absolute_error: 0.2550 - val_loss: 0.0500 - val_mean_absolute_error: 0.1835\n",
      "Epoch 11/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1027 - mean_absolute_error: 0.2481 - val_loss: 0.0622 - val_mean_absolute_error: 0.2045\n",
      "Epoch 12/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1009 - mean_absolute_error: 0.2417 - val_loss: 0.0464 - val_mean_absolute_error: 0.1753\n",
      "Epoch 13/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0876 - mean_absolute_error: 0.2317 - val_loss: 0.0415 - val_mean_absolute_error: 0.1641\n",
      "Epoch 14/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0897 - mean_absolute_error: 0.2276 - val_loss: 0.0492 - val_mean_absolute_error: 0.1790\n",
      "Epoch 15/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0867 - mean_absolute_error: 0.2242 - val_loss: 0.0449 - val_mean_absolute_error: 0.1726\n",
      "Epoch 16/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0905 - mean_absolute_error: 0.2346 - val_loss: 0.0461 - val_mean_absolute_error: 0.1748\n",
      "Epoch 17/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0795 - mean_absolute_error: 0.2192 - val_loss: 0.0381 - val_mean_absolute_error: 0.1554\n",
      "Epoch 18/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0864 - mean_absolute_error: 0.2232 - val_loss: 0.0363 - val_mean_absolute_error: 0.1536\n",
      "Epoch 19/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0764 - mean_absolute_error: 0.2154 - val_loss: 0.0406 - val_mean_absolute_error: 0.1594\n",
      "Epoch 20/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0761 - mean_absolute_error: 0.2140 - val_loss: 0.0551 - val_mean_absolute_error: 0.1919\n",
      "Epoch 21/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0708 - mean_absolute_error: 0.1982 - val_loss: 0.0389 - val_mean_absolute_error: 0.1569\n",
      "Epoch 22/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0738 - mean_absolute_error: 0.2097 - val_loss: 0.0397 - val_mean_absolute_error: 0.1566\n",
      "Epoch 23/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0701 - mean_absolute_error: 0.2022 - val_loss: 0.0399 - val_mean_absolute_error: 0.1597\n",
      "Epoch 24/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0670 - mean_absolute_error: 0.2020 - val_loss: 0.0411 - val_mean_absolute_error: 0.1625\n",
      "Epoch 25/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0741 - mean_absolute_error: 0.2140 - val_loss: 0.0402 - val_mean_absolute_error: 0.1587\n",
      "Epoch 26/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0625 - mean_absolute_error: 0.1993 - val_loss: 0.0389 - val_mean_absolute_error: 0.1556\n",
      "Epoch 27/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0671 - mean_absolute_error: 0.1994 - val_loss: 0.0423 - val_mean_absolute_error: 0.1633\n",
      "Epoch 28/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0632 - mean_absolute_error: 0.1965 - val_loss: 0.0420 - val_mean_absolute_error: 0.1661\n",
      "Epoch 29/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0738 - mean_absolute_error: 0.2093 - val_loss: 0.0361 - val_mean_absolute_error: 0.1494\n",
      "Epoch 30/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0644 - mean_absolute_error: 0.1936 - val_loss: 0.0475 - val_mean_absolute_error: 0.1738\n",
      "Epoch 31/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0682 - mean_absolute_error: 0.2012 - val_loss: 0.0351 - val_mean_absolute_error: 0.1484\n",
      "Epoch 32/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0513 - mean_absolute_error: 0.1753 - val_loss: 0.0419 - val_mean_absolute_error: 0.1624\n",
      "Epoch 33/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0545 - mean_absolute_error: 0.1770 - val_loss: 0.0355 - val_mean_absolute_error: 0.1487\n",
      "Epoch 34/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0608 - mean_absolute_error: 0.1931 - val_loss: 0.0417 - val_mean_absolute_error: 0.1609\n",
      "Epoch 35/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0559 - mean_absolute_error: 0.1814 - val_loss: 0.0408 - val_mean_absolute_error: 0.1582\n",
      "Epoch 36/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0576 - mean_absolute_error: 0.1850 - val_loss: 0.0388 - val_mean_absolute_error: 0.1522\n",
      "Epoch 37/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0539 - mean_absolute_error: 0.1743 - val_loss: 0.0379 - val_mean_absolute_error: 0.1523\n",
      "Epoch 38/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0550 - mean_absolute_error: 0.1788 - val_loss: 0.0383 - val_mean_absolute_error: 0.1531\n",
      "Epoch 39/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0572 - mean_absolute_error: 0.1800 - val_loss: 0.0489 - val_mean_absolute_error: 0.1721\n",
      "Epoch 40/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0555 - mean_absolute_error: 0.1827 - val_loss: 0.0367 - val_mean_absolute_error: 0.1484\n",
      "Epoch 41/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0561 - mean_absolute_error: 0.1828 - val_loss: 0.0483 - val_mean_absolute_error: 0.1729\n",
      "Epoch 42/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0548 - mean_absolute_error: 0.1817 - val_loss: 0.0389 - val_mean_absolute_error: 0.1544\n",
      "Epoch 43/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0531 - mean_absolute_error: 0.1821 - val_loss: 0.0403 - val_mean_absolute_error: 0.1570\n",
      "Epoch 44/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0557 - mean_absolute_error: 0.1828 - val_loss: 0.0383 - val_mean_absolute_error: 0.1564\n",
      "Epoch 45/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0537 - mean_absolute_error: 0.1783 - val_loss: 0.0379 - val_mean_absolute_error: 0.1544\n",
      "Epoch 46/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0543 - mean_absolute_error: 0.1777 - val_loss: 0.0361 - val_mean_absolute_error: 0.1486\n",
      "Epoch 47/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0512 - mean_absolute_error: 0.1717 - val_loss: 0.0510 - val_mean_absolute_error: 0.1793\n",
      "Epoch 48/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0541 - mean_absolute_error: 0.1795 - val_loss: 0.0543 - val_mean_absolute_error: 0.1855\n",
      "Epoch 49/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0527 - mean_absolute_error: 0.1764 - val_loss: 0.0397 - val_mean_absolute_error: 0.1546\n",
      "Epoch 50/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0477 - mean_absolute_error: 0.1695 - val_loss: 0.0355 - val_mean_absolute_error: 0.1448\n",
      "Epoch 51/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0493 - mean_absolute_error: 0.1713 - val_loss: 0.0414 - val_mean_absolute_error: 0.1613\n",
      "Epoch 52/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0522 - mean_absolute_error: 0.1780 - val_loss: 0.0385 - val_mean_absolute_error: 0.1529\n",
      "Epoch 53/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0482 - mean_absolute_error: 0.1713 - val_loss: 0.0367 - val_mean_absolute_error: 0.1479\n",
      "Epoch 54/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0552 - mean_absolute_error: 0.1808 - val_loss: 0.0401 - val_mean_absolute_error: 0.1563\n",
      "Epoch 55/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0508 - mean_absolute_error: 0.1730 - val_loss: 0.0400 - val_mean_absolute_error: 0.1547\n",
      "Epoch 56/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0468 - mean_absolute_error: 0.1709 - val_loss: 0.0364 - val_mean_absolute_error: 0.1471\n",
      "Epoch 57/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0503 - mean_absolute_error: 0.1747 - val_loss: 0.0429 - val_mean_absolute_error: 0.1628\n",
      "Epoch 58/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0451 - mean_absolute_error: 0.1662 - val_loss: 0.0356 - val_mean_absolute_error: 0.1459\n",
      "Epoch 59/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0483 - mean_absolute_error: 0.1699 - val_loss: 0.0398 - val_mean_absolute_error: 0.1545\n",
      "Epoch 60/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0514 - mean_absolute_error: 0.1758 - val_loss: 0.0472 - val_mean_absolute_error: 0.1700\n",
      "Epoch 61/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0452 - mean_absolute_error: 0.1655 - val_loss: 0.0429 - val_mean_absolute_error: 0.1579\n",
      "Epoch 62/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0449 - mean_absolute_error: 0.1665 - val_loss: 0.0434 - val_mean_absolute_error: 0.1600\n",
      "Epoch 63/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0519 - mean_absolute_error: 0.1732 - val_loss: 0.0662 - val_mean_absolute_error: 0.2054\n",
      "Epoch 64/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0462 - mean_absolute_error: 0.1650 - val_loss: 0.0486 - val_mean_absolute_error: 0.1726\n",
      "Epoch 65/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0464 - mean_absolute_error: 0.1659 - val_loss: 0.0511 - val_mean_absolute_error: 0.1758\n",
      "Epoch 66/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0547 - mean_absolute_error: 0.1766 - val_loss: 0.0423 - val_mean_absolute_error: 0.1571\n",
      "Epoch 67/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0484 - mean_absolute_error: 0.1710 - val_loss: 0.0415 - val_mean_absolute_error: 0.1559\n",
      "Epoch 68/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0436 - mean_absolute_error: 0.1604 - val_loss: 0.0490 - val_mean_absolute_error: 0.1674\n",
      "Epoch 69/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0505 - mean_absolute_error: 0.1730 - val_loss: 0.0486 - val_mean_absolute_error: 0.1731\n",
      "Epoch 70/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0494 - mean_absolute_error: 0.1695 - val_loss: 0.0441 - val_mean_absolute_error: 0.1582\n",
      "Epoch 71/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0459 - mean_absolute_error: 0.1633 - val_loss: 0.0535 - val_mean_absolute_error: 0.1794\n",
      "Epoch 72/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0461 - mean_absolute_error: 0.1643 - val_loss: 0.0391 - val_mean_absolute_error: 0.1508\n",
      "Epoch 73/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0530 - mean_absolute_error: 0.1735 - val_loss: 0.0416 - val_mean_absolute_error: 0.1566\n",
      "Epoch 74/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0466 - mean_absolute_error: 0.1684 - val_loss: 0.0380 - val_mean_absolute_error: 0.1472\n",
      "Epoch 75/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0446 - mean_absolute_error: 0.1636 - val_loss: 0.0433 - val_mean_absolute_error: 0.1588\n",
      "Epoch 76/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0472 - mean_absolute_error: 0.1667 - val_loss: 0.0509 - val_mean_absolute_error: 0.1718\n",
      "Epoch 77/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0455 - mean_absolute_error: 0.1629 - val_loss: 0.0374 - val_mean_absolute_error: 0.1473\n",
      "Epoch 78/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0438 - mean_absolute_error: 0.1631 - val_loss: 0.0413 - val_mean_absolute_error: 0.1539\n",
      "Epoch 79/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0455 - mean_absolute_error: 0.1619 - val_loss: 0.0396 - val_mean_absolute_error: 0.1531\n",
      "Epoch 80/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0467 - mean_absolute_error: 0.1647 - val_loss: 0.0460 - val_mean_absolute_error: 0.1660\n",
      "Epoch 81/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0468 - mean_absolute_error: 0.1663 - val_loss: 0.0430 - val_mean_absolute_error: 0.1550\n",
      "Epoch 82/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0445 - mean_absolute_error: 0.1622 - val_loss: 0.0452 - val_mean_absolute_error: 0.1611\n",
      "Epoch 83/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0429 - mean_absolute_error: 0.1610 - val_loss: 0.0427 - val_mean_absolute_error: 0.1586\n",
      "Epoch 84/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0456 - mean_absolute_error: 0.1613 - val_loss: 0.0388 - val_mean_absolute_error: 0.1512\n",
      "Epoch 85/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0386 - mean_absolute_error: 0.1547 - val_loss: 0.0452 - val_mean_absolute_error: 0.1601\n",
      "Epoch 86/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0501 - mean_absolute_error: 0.1716 - val_loss: 0.0421 - val_mean_absolute_error: 0.1563\n",
      "Epoch 87/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0432 - mean_absolute_error: 0.1595 - val_loss: 0.0391 - val_mean_absolute_error: 0.1505\n",
      "Epoch 88/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0442 - mean_absolute_error: 0.1602 - val_loss: 0.0378 - val_mean_absolute_error: 0.1513\n",
      "Epoch 89/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0476 - mean_absolute_error: 0.1691 - val_loss: 0.0399 - val_mean_absolute_error: 0.1531\n",
      "Epoch 90/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0444 - mean_absolute_error: 0.1604 - val_loss: 0.0464 - val_mean_absolute_error: 0.1656\n",
      "Epoch 91/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0454 - mean_absolute_error: 0.1639 - val_loss: 0.0389 - val_mean_absolute_error: 0.1522\n",
      "Epoch 92/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0463 - mean_absolute_error: 0.1670 - val_loss: 0.0419 - val_mean_absolute_error: 0.1575\n",
      "Epoch 93/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0429 - mean_absolute_error: 0.1612 - val_loss: 0.0461 - val_mean_absolute_error: 0.1638\n",
      "Epoch 94/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0456 - mean_absolute_error: 0.1627 - val_loss: 0.0414 - val_mean_absolute_error: 0.1598\n",
      "Epoch 95/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0433 - mean_absolute_error: 0.1563 - val_loss: 0.0374 - val_mean_absolute_error: 0.1498\n",
      "Epoch 96/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0395 - mean_absolute_error: 0.1546 - val_loss: 0.0404 - val_mean_absolute_error: 0.1553\n",
      "Epoch 97/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0494 - mean_absolute_error: 0.1685 - val_loss: 0.0373 - val_mean_absolute_error: 0.1494\n",
      "Epoch 98/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0448 - mean_absolute_error: 0.1624 - val_loss: 0.0366 - val_mean_absolute_error: 0.1470\n",
      "Epoch 99/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0448 - mean_absolute_error: 0.1614 - val_loss: 0.0418 - val_mean_absolute_error: 0.1571\n",
      "Epoch 100/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0419 - mean_absolute_error: 0.1591 - val_loss: 0.0397 - val_mean_absolute_error: 0.1556\n",
      "Epoch 101/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0475 - mean_absolute_error: 0.1692 - val_loss: 0.0362 - val_mean_absolute_error: 0.1473\n",
      "Epoch 102/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0463 - mean_absolute_error: 0.1661 - val_loss: 0.0415 - val_mean_absolute_error: 0.1602\n",
      "Epoch 103/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0396 - mean_absolute_error: 0.1544 - val_loss: 0.0418 - val_mean_absolute_error: 0.1588\n",
      "Epoch 104/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0415 - mean_absolute_error: 0.1561 - val_loss: 0.0407 - val_mean_absolute_error: 0.1542\n",
      "Epoch 105/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0405 - mean_absolute_error: 0.1533 - val_loss: 0.0439 - val_mean_absolute_error: 0.1601\n",
      "Epoch 106/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0442 - mean_absolute_error: 0.1601 - val_loss: 0.0468 - val_mean_absolute_error: 0.1710\n",
      "Epoch 107/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0464 - mean_absolute_error: 0.1657 - val_loss: 0.0409 - val_mean_absolute_error: 0.1595\n",
      "Epoch 108/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0455 - mean_absolute_error: 0.1660 - val_loss: 0.0410 - val_mean_absolute_error: 0.1545\n",
      "Epoch 109/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0402 - mean_absolute_error: 0.1561 - val_loss: 0.0380 - val_mean_absolute_error: 0.1503\n",
      "Epoch 110/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0413 - mean_absolute_error: 0.1571 - val_loss: 0.0415 - val_mean_absolute_error: 0.1558\n",
      "Epoch 111/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0446 - mean_absolute_error: 0.1649 - val_loss: 0.0388 - val_mean_absolute_error: 0.1535\n",
      "Epoch 112/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0438 - mean_absolute_error: 0.1636 - val_loss: 0.0380 - val_mean_absolute_error: 0.1527\n",
      "Epoch 113/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0433 - mean_absolute_error: 0.1604 - val_loss: 0.0392 - val_mean_absolute_error: 0.1524\n",
      "Epoch 114/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0383 - mean_absolute_error: 0.1506 - val_loss: 0.0420 - val_mean_absolute_error: 0.1594\n",
      "Epoch 115/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0453 - mean_absolute_error: 0.1577 - val_loss: 0.0406 - val_mean_absolute_error: 0.1562\n",
      "Epoch 116/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0432 - mean_absolute_error: 0.1607 - val_loss: 0.0415 - val_mean_absolute_error: 0.1567\n",
      "Epoch 117/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0403 - mean_absolute_error: 0.1545 - val_loss: 0.0492 - val_mean_absolute_error: 0.1700\n",
      "Epoch 118/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0442 - mean_absolute_error: 0.1565 - val_loss: 0.0518 - val_mean_absolute_error: 0.1747\n",
      "Epoch 119/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0462 - mean_absolute_error: 0.1603 - val_loss: 0.0399 - val_mean_absolute_error: 0.1543\n",
      "Epoch 120/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0443 - mean_absolute_error: 0.1584 - val_loss: 0.0398 - val_mean_absolute_error: 0.1536\n",
      "Epoch 121/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0419 - mean_absolute_error: 0.1546 - val_loss: 0.0500 - val_mean_absolute_error: 0.1764\n",
      "Epoch 122/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0466 - mean_absolute_error: 0.1666 - val_loss: 0.0403 - val_mean_absolute_error: 0.1578\n",
      "Epoch 123/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0431 - mean_absolute_error: 0.1618 - val_loss: 0.0392 - val_mean_absolute_error: 0.1554\n",
      "Epoch 124/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0395 - mean_absolute_error: 0.1537 - val_loss: 0.0419 - val_mean_absolute_error: 0.1606\n",
      "Epoch 125/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0415 - mean_absolute_error: 0.1579 - val_loss: 0.0490 - val_mean_absolute_error: 0.1739\n",
      "Epoch 126/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0430 - mean_absolute_error: 0.1613 - val_loss: 0.0446 - val_mean_absolute_error: 0.1643\n",
      "Epoch 127/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0434 - mean_absolute_error: 0.1569 - val_loss: 0.0447 - val_mean_absolute_error: 0.1638\n",
      "Epoch 128/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0445 - mean_absolute_error: 0.1592 - val_loss: 0.0407 - val_mean_absolute_error: 0.1599\n",
      "Epoch 129/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0406 - mean_absolute_error: 0.1531 - val_loss: 0.0451 - val_mean_absolute_error: 0.1644\n",
      "Epoch 130/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0449 - mean_absolute_error: 0.1618 - val_loss: 0.0422 - val_mean_absolute_error: 0.1630\n",
      "Epoch 131/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0459 - mean_absolute_error: 0.1663 - val_loss: 0.0466 - val_mean_absolute_error: 0.1679\n",
      "Epoch 132/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0438 - mean_absolute_error: 0.1630 - val_loss: 0.0429 - val_mean_absolute_error: 0.1634\n",
      "Epoch 133/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0427 - mean_absolute_error: 0.1579 - val_loss: 0.0478 - val_mean_absolute_error: 0.1739\n",
      "Epoch 134/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0427 - mean_absolute_error: 0.1592 - val_loss: 0.0441 - val_mean_absolute_error: 0.1635\n",
      "Epoch 135/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0416 - mean_absolute_error: 0.1556 - val_loss: 0.0468 - val_mean_absolute_error: 0.1663\n",
      "Epoch 136/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0401 - mean_absolute_error: 0.1577 - val_loss: 0.0426 - val_mean_absolute_error: 0.1599\n",
      "Epoch 137/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0421 - mean_absolute_error: 0.1603 - val_loss: 0.0454 - val_mean_absolute_error: 0.1662\n",
      "Epoch 138/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0437 - mean_absolute_error: 0.1579 - val_loss: 0.0414 - val_mean_absolute_error: 0.1595\n",
      "Epoch 139/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0405 - mean_absolute_error: 0.1554 - val_loss: 0.0447 - val_mean_absolute_error: 0.1635\n",
      "Epoch 140/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0431 - mean_absolute_error: 0.1571 - val_loss: 0.0453 - val_mean_absolute_error: 0.1647\n",
      "Epoch 141/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0434 - mean_absolute_error: 0.1606 - val_loss: 0.0427 - val_mean_absolute_error: 0.1627\n",
      "Epoch 142/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0361 - mean_absolute_error: 0.1462 - val_loss: 0.0425 - val_mean_absolute_error: 0.1604\n",
      "Epoch 143/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0414 - mean_absolute_error: 0.1551 - val_loss: 0.0399 - val_mean_absolute_error: 0.1554\n",
      "Epoch 144/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0376 - mean_absolute_error: 0.1531 - val_loss: 0.0403 - val_mean_absolute_error: 0.1527\n",
      "Epoch 145/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0413 - mean_absolute_error: 0.1552 - val_loss: 0.0428 - val_mean_absolute_error: 0.1614\n",
      "Epoch 146/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0417 - mean_absolute_error: 0.1535 - val_loss: 0.0420 - val_mean_absolute_error: 0.1584\n",
      "Epoch 147/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0458 - mean_absolute_error: 0.1636 - val_loss: 0.0441 - val_mean_absolute_error: 0.1624\n",
      "Epoch 148/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0405 - mean_absolute_error: 0.1534 - val_loss: 0.0467 - val_mean_absolute_error: 0.1697\n",
      "Epoch 149/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0469 - mean_absolute_error: 0.1612 - val_loss: 0.0412 - val_mean_absolute_error: 0.1580\n",
      "Epoch 150/150\n",
      "\u001b[1m306/306\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0409 - mean_absolute_error: 0.1539 - val_loss: 0.0391 - val_mean_absolute_error: 0.1541\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0405 - mean_absolute_error: 0.1575 \n",
      "Loss: 0.03990212082862854\n",
      "MSE: 0.16056938469409943\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "# Dataset Data Engineering\n",
    "dataset3 = data.drop(columns=['StudentID'])\n",
    "X3 = dataset3.drop(\"GPA\", axis=1)\n",
    "y3 = dataset3[[\"GPA\"]]\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.2, random_state=42)\n",
    "scaler3 = StandardScaler()\n",
    "X_train3 = scaler3.fit_transform(X_train3)\n",
    "X_test3 = scaler3.transform(X_test3)\n",
    "\n",
    "# Your code here\n",
    "model3 = Sequential([\n",
    "    Dense(64, input_dim=13, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Model Compile\n",
    "model3.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Model Training\n",
    "model3.fit(X_train3, y_train3, batch_size=5, epochs=150, validation_split=0.2)\n",
    "\n",
    "# Model Evaluation\n",
    "loss3, mse3 = model3.evaluate(X_test3, y_test3)\n",
    "print(\"Loss:\", loss3)\n",
    "print(\"MSE:\", mse3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3rd model is the best with the lowest squared and absolute error and this is due to the dropout layer to avoid overfitting by including extra columns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
